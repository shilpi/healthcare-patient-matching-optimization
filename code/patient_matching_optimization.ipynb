{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Patient Matching Algorithms with Bayesian Optimization\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the application of Bayesian Optimization techniques to tune patient matching algorithms for an Enterprise Master Patient Index (eMPI) solution. Patient matching is a critical component of healthcare interoperability, as it enables the accurate linking of patient records across disparate systems.\n",
    "\n",
    "Traditional approaches to patient matching parameter tuning often rely on manual trial-and-error or grid search methods, which are inefficient and may not find optimal configurations. Bayesian Optimization provides a more efficient approach by modeling the relationship between parameters and matching performance, then strategically selecting new parameter combinations to evaluate.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Load and prepare synthetic patient data from Synthea\n",
    "2. Implement a patient matching algorithm with configurable parameters\n",
    "3. Define an objective function that evaluates matching performance\n",
    "4. Apply Bayesian Optimization to find optimal parameter settings\n",
    "5. Analyze and visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, RBF, ConstantKernel\n",
    "from scipy.optimize import minimize\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/csv/patients.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the Synthea patient data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m patients_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/csv/patients.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Display the first few rows to understand the data structure\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal number of patients: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(patients_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/csv/patients.csv'"
     ]
    }
   ],
   "source": [
    "# Load the Synthea patient data\n",
    "patients_df = pd.read_csv('../data/csv/patients.csv')\n",
    "\n",
    "# Display the first few rows to understand the data structure\n",
    "print(f\"Total number of patients: {len(patients_df)}\")\n",
    "patients_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation for Patient Matching\n",
    "\n",
    "To simulate a realistic patient matching scenario, we need to:\n",
    "1. Select relevant demographic fields used for matching\n",
    "2. Create duplicate records with various types of errors to simulate real-world data quality issues\n",
    "3. Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant fields for patient matching\n",
    "matching_fields = ['Id', 'BIRTHDATE', 'FIRST', 'LAST', 'MAIDEN', 'GENDER', 'ADDRESS', 'CITY', 'STATE', 'ZIP']\n",
    "patients_subset = patients_df[matching_fields].copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "patients_subset = patients_subset.rename(columns={\n",
    "    'Id': 'patient_id',\n",
    "    'BIRTHDATE': 'birth_date',\n",
    "    'FIRST': 'first_name',\n",
    "    'LAST': 'last_name',\n",
    "    'MAIDEN': 'maiden_name',\n",
    "    'GENDER': 'gender',\n",
    "    'ADDRESS': 'address',\n",
    "    'CITY': 'city',\n",
    "    'STATE': 'state',\n",
    "    'ZIP': 'zip'\n",
    "})\n",
    "\n",
    "# Display the cleaned subset\n",
    "patients_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_error(value, error_type='typo', severity=0.3):\n",
    "    \"\"\"Introduce realistic errors into string values to simulate data quality issues.\n",
    "    \n",
    "    Args:\n",
    "        value (str): The original string value\n",
    "        error_type (str): Type of error to introduce ('typo', 'missing', 'swap', 'extra')\n",
    "        severity (float): Probability of introducing an error (0.0 to 1.0)\n",
    "        \n",
    "    Returns:\n",
    "        str: The modified string with introduced errors\n",
    "    \"\"\"\n",
    "    if not isinstance(value, str) or not value or random.random() > severity:\n",
    "        return value\n",
    "    \n",
    "    if error_type == 'typo':\n",
    "        # Replace a random character with another character\n",
    "        if len(value) > 1:\n",
    "            pos = random.randint(0, len(value) - 1)\n",
    "            replacement = random.choice(string.ascii_letters + string.digits)\n",
    "            return value[:pos] + replacement + value[pos+1:]\n",
    "    \n",
    "    elif error_type == 'missing':\n",
    "        # Remove a random character\n",
    "        if len(value) > 1:\n",
    "            pos = random.randint(0, len(value) - 1)\n",
    "            return value[:pos] + value[pos+1:]\n",
    "    \n",
    "    elif error_type == 'swap':\n",
    "        # Swap two adjacent characters\n",
    "        if len(value) > 2:\n",
    "            pos = random.randint(0, len(value) - 2)\n",
    "            return value[:pos] + value[pos+1] + value[pos] + value[pos+2:]\n",
    "    \n",
    "    elif error_type == 'extra':\n",
    "        # Insert an extra character\n",
    "        pos = random.randint(0, len(value))\n",
    "        extra_char = random.choice(string.ascii_letters + string.digits)\n",
    "        return value[:pos] + extra_char + value[pos:]\n",
    "    \n",
    "    return value\n",
    "\n",
    "def create_duplicate_with_errors(record, error_fields, error_types, error_severity):\n",
    "    \"\"\"Create a duplicate record with introduced errors in specified fields.\n",
    "    \n",
    "    Args:\n",
    "        record (pd.Series): Original patient record\n",
    "        error_fields (list): List of field names to potentially introduce errors\n",
    "        error_types (list): List of error types to choose from\n",
    "        error_severity (float): Probability of introducing an error in each field\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: Modified duplicate record with introduced errors\n",
    "    \"\"\"\n",
    "    duplicate = record.copy()\n",
    "    \n",
    "    # Generate a new ID for the duplicate record\n",
    "    duplicate['patient_id'] = f\"DUP-{record['patient_id']}\"\n",
    "    \n",
    "    # Introduce errors in specified fields\n",
    "    for field in error_fields:\n",
    "        if field in duplicate.index and isinstance(duplicate[field], str):\n",
    "            error_type = random.choice(error_types)\n",
    "            duplicate[field] = introduce_error(duplicate[field], error_type, error_severity)\n",
    "    \n",
    "    return duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with duplicate records containing various errors\n",
    "def generate_matching_dataset(patients_df, duplicate_ratio=0.3, error_severity=0.5):\n",
    "    \"\"\"Generate a dataset with original and duplicate records for patient matching.\n",
    "    \n",
    "    Args:\n",
    "        patients_df (pd.DataFrame): Original patient records\n",
    "        duplicate_ratio (float): Proportion of records to duplicate with errors\n",
    "        error_severity (float): Probability of introducing errors in each field\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (combined_df, match_pairs) where combined_df contains all records and\n",
    "               match_pairs is a list of (original_id, duplicate_id) pairs\n",
    "    \"\"\"\n",
    "    # Sample records to create duplicates with errors\n",
    "    sample_size = int(len(patients_df) * duplicate_ratio)\n",
    "    sample_indices = random.sample(range(len(patients_df)), sample_size)\n",
    "    \n",
    "    # Fields that can have errors introduced\n",
    "    error_fields = ['first_name', 'last_name', 'maiden_name', 'address', 'city', 'zip']\n",
    "    error_types = ['typo', 'missing', 'swap', 'extra']\n",
    "    \n",
    "    # Create duplicates with errors\n",
    "    duplicates = []\n",
    "    match_pairs = []\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        original_record = patients_df.iloc[idx]\n",
    "        duplicate_record = create_duplicate_with_errors(\n",
    "            original_record, error_fields, error_types, error_severity)\n",
    "        \n",
    "        duplicates.append(duplicate_record)\n",
    "        match_pairs.append((original_record['patient_id'], duplicate_record['patient_id']))\n",
    "    \n",
    "    # Combine original records with duplicates\n",
    "    duplicates_df = pd.DataFrame(duplicates)\n",
    "    combined_df = pd.concat([patients_df, duplicates_df], ignore_index=True)\n",
    "    \n",
    "    return combined_df, match_pairs\n",
    "\n",
    "# Generate the matching dataset\n",
    "matching_data, true_matches = generate_matching_dataset(patients_subset, duplicate_ratio=0.3, error_severity=0.5)\n",
    "\n",
    "# Create a dictionary for quick lookup of true matches\n",
    "true_match_dict = {}\n",
    "for orig_id, dup_id in true_matches:\n",
    "    true_match_dict[orig_id] = dup_id\n",
    "    true_match_dict[dup_id] = orig_id\n",
    "\n",
    "print(f\"Total records in matching dataset: {len(matching_data)}\")\n",
    "print(f\"Number of true match pairs: {len(true_matches)}\")\n",
    "\n",
    "# Display a few examples of original records and their duplicates with errors\n",
    "for i in range(3):\n",
    "    orig_id, dup_id = true_matches[i]\n",
    "    orig_record = matching_data[matching_data['patient_id'] == orig_id].iloc[0]\n",
    "    dup_record = matching_data[matching_data['patient_id'] == dup_id].iloc[0]\n",
    "    \n",
    "    print(f\"\\nMatch Pair {i+1}:\")\n",
    "    print(f\"Original: {dict(orig_record[['first_name', 'last_name', 'birth_date', 'address']])}\")\n",
    "    print(f\"Duplicate: {dict(dup_record[['first_name', 'last_name', 'birth_date', 'address']])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(matching_data, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Testing set size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Patient Matching Algorithm Implementation\n",
    "\n",
    "Now we'll implement a configurable patient matching algorithm that can be optimized using Bayesian Optimization. The algorithm will:\n",
    "\n",
    "1. Calculate similarity scores between patient records using various comparison methods\n",
    "2. Apply field-specific weights to the similarity scores\n",
    "3. Determine matches based on a configurable threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(s1, s2):\n",
    "    \"\"\"Calculate the Levenshtein (edit) distance between two strings.\n",
    "    \n",
    "    Args:\n",
    "        s1 (str): First string\n",
    "        s2 (str): Second string\n",
    "        \n",
    "    Returns:\n",
    "        int: The edit distance between the strings\n",
    "    \"\"\"\n",
    "    if not isinstance(s1, str) or not isinstance(s2, str):\n",
    "        return float('inf')\n",
    "    \n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "    \n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    \n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]\n",
    "\n",
    "def levenshtein_similarity(s1, s2):\n",
    "    \"\"\"Calculate similarity based on Levenshtein distance, normalized to [0,1].\n",
    "    \n",
    "    Args:\n",
    "        s1 (str): First string\n",
    "        s2 (str): Second string\n",
    "        \n",
    "    Returns:\n",
    "        float: Similarity score between 0 (completely different) and 1 (identical)\n",
    "    \"\"\"\n",
    "    if not isinstance(s1, str) or not isinstance(s2, str):\n",
    "        return 0.0\n",
    "    \n",
    "    if not s1 and not s2:\n",
    "        return 1.0\n",
    "    \n",
    "    if not s1 or not s2:\n",
    "        return 0.0\n",
    "    \n",
    "    distance = levenshtein_distance(s1, s2)\n",
    "    max_len = max(len(s1), len(s2))\n",
    "    return 1 - (distance / max_len)\n",
    "\n",
    "def jaro_similarity(s1, s2):\n",
    "    \"\"\"Calculate Jaro similarity between two strings.\n",
    "    \n",
    "    Args:\n",
    "        s1 (str): First string\n",
    "        s2 (str): Second string\n",
    "        \n",
    "    Returns:\n",
    "        float: Similarity score between 0 (completely different) and 1 (identical)\n",
    "    \"\"\"\n",
    "    if not isinstance(s1, str) or not isinstance(s2, str):\n",
    "        return 0.0\n",
    "    \n",
    "    if not s1 and not s2:\n",
    "        return 1.0\n",
    "    \n",
    "    if not s1 or not s2:\n",
    "        return 0.0\n",
    "    \n",
    "    # If the strings are identical, return 1.0\n",
    "    if s1 == s2:\n",
    "        return 1.0\n",
    "    \n",
    "    # Ensure s1 is the shorter string\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "    \n",
    "    # Maximum distance for matching characters\n",
    "    match_distance = max(len(s1), len(s2)) // 2 - 1\n",
    "    match_distance = max(0, match_distance)\n",
    "    \n",
    "    # Find matching characters\n",
    "    s1_matches = [False] * len(s1)\n",
    "    s2_matches = [False] * len(s2)\n",
    "    \n",
    "    matches = 0\n",
    "    for i in range(len(s1)):\n",
    "        start = max(0, i - match_distance)\n",
    "        end = min(i + match_distance + 1, len(s2))\n",
    "        \n",
    "        for j in range(start, end):\n",
    "            if not s2_matches[j] and s1[i] == s2[j]:\n",
    "                s1_matches[i] = True\n",
    "                s2_matches[j] = True\n",
    "                matches += 1\n",
    "                break\n",
    "    \n",
    "    if matches == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Count transpositions\n",
    "    transpositions = 0\n",
    "    k = 0\n",
    "    \n",
    "    for i in range(len(s1)):\n",
    "        if s1_matches[i]:\n",
    "            while not s2_matches[k]:\n",
    "                k += 1\n",
    "            if s1[i] != s2[k]:\n",
    "                transpositions += 1\n",
    "            k += 1\n",
    "    \n",
    "    transpositions = transpositions // 2\n",
    "    \n",
    "    # Calculate Jaro similarity\n",
    "    return (matches / len(s1) + matches / len(s2) + (matches - transpositions) / matches) / 3.0\n",
    "\n",
    "def date_similarity(date1, date2):\n",
    "    \"\"\"Calculate similarity between two dates.\n",
    "    \n",
    "    Args:\n",
    "        date1 (str): First date string in format YYYY-MM-DD\n",
    "        date2 (str): Second date string in format YYYY-MM-DD\n",
    "        \n",
    "    Returns:\n",
    "        float: Similarity score between 0 (completely different) and 1 (identical)\n",
    "    \"\"\"\n",
    "    if not isinstance(date1, str) or not isinstance(date2, str):\n",
    "        return 0.0\n",
    "    \n",
    "    if not date1 or not date2:\n",
    "        return 0.0\n",
    "    \n",
    "    if date1 == date2:\n",
    "        return 1.0\n",
    "    \n",
    "    try:\n",
    "        # Parse dates\n",
    "        year1, month1, day1 = date1.split('-')\n",
    "        year2, month2, day2 = date2.split('-')\n",
    "        \n",
    "        # Calculate component similarities\n",
    "        year_sim = 1.0 if year1 == year2 else 0.0\n",
    "        month_sim = 1.0 if month1 == month2 else 0.0\n",
    "        day_sim = 1.0 if day1 == day2 else 0.0\n",
    "        \n",
    "        # Weight year more heavily than month and day\n",
    "        return 0.6 * year_sim + 0.2 * month_sim + 0.2 * day_sim\n",
    "    except:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_record_similarity(record1, record2, field_weights, similarity_funcs):\n",
    "    \"\"\"Calculate the weighted similarity between two patient records.\n",
    "    \n",
    "    Args:\n",
    "        record1 (pd.Series): First patient record\n",
    "        record2 (pd.Series): Second patient record\n",
    "        field_weights (dict): Dictionary mapping field names to their weights\n",
    "        similarity_funcs (dict): Dictionary mapping field names to similarity functions\n",
    "        \n",
    "    Returns:\n",
    "        float: Weighted similarity score between 0 and 1\n",
    "    \"\"\"\n",
    "    total_weight = 0.0\n",
    "    weighted_similarity = 0.0\n",
    "    \n",
    "    for field, weight in field_weights.items():\n",
    "        if field in record1 and field in record2:\n",
    "            # Skip patient_id field\n",
    "            if field == 'patient_id':\n",
    "                continue\n",
    "                \n",
    "            # Get the appropriate similarity function for this field\n",
    "            sim_func = similarity_funcs.get(field, levenshtein_similarity)\n",
    "            \n",
    "            # Calculate similarity for this field\n",
    "            field_similarity = sim_func(str(record1[field]), str(record2[field]))\n",
    "            \n",
    "            # Add to weighted similarity\n",
    "            weighted_similarity += weight * field_similarity\n",
    "            total_weight += weight\n",
    "    \n",
    "    # Normalize by total weight\n",
    "    if total_weight > 0:\n",
    "        return weighted_similarity / total_weight\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def find_matches(data, field_weights, similarity_funcs, threshold, blocking_field=None):\n",
    "    \"\"\"Find matching patient records in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Patient records dataset\n",
    "        field_weights (dict): Dictionary mapping field names to their weights\n",
    "        similarity_funcs (dict): Dictionary mapping field names to similarity functions\n",
    "        threshold (float): Minimum similarity score to consider a match\n",
    "        blocking_field (str, optional): Field to use for blocking to reduce comparisons\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tuples (id1, id2, similarity) for predicted matches\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    \n",
    "    # Create blocks if blocking field is specified\n",
    "    if blocking_field and blocking_field in data.columns:\n",
    "        blocks = {}\n",
    "        for _, record in data.iterrows():\n",
    "            block_key = record[blocking_field]\n",
    "            if block_key not in blocks:\n",
    "                blocks[block_key] = []\n",
    "            blocks[block_key].append(record)\n",
    "        \n",
    "        # Compare records within each block\n",
    "        for block_key, block_records in blocks.items():\n",
    "            for i in range(len(block_records)):\n",
    "                for j in range(i+1, len(block_records)):\n",
    "                    record1 = block_records[i]\n",
    "                    record2 = block_records[j]\n",
    "                    \n",
    "                    # Skip comparing a record with itself\n",
    "                    if record1['patient_id'] == record2['patient_id']:\n",
    "                        continue\n",
    "                    \n",
    "                    similarity = calculate_record_similarity(\n",
    "                        record1, record2, field_weights, similarity_funcs)\n",
    "                    \n",
    "                    if similarity >= threshold:\n",
    "                        matches.append((record1['patient_id'], record2['patient_id'], similarity))\n",
    "    else:\n",
    "        # Compare all records (less efficient)\n",
    "        for i in range(len(data)):\n",
    "            for j in range(i+1, len(data)):\n",
    "                record1 = data.iloc[i]\n",
    "                record2 = data.iloc[j]\n",
    "                \n",
    "                similarity = calculate_record_similarity(\n",
    "                    record1, record2, field_weights, similarity_funcs)\n",
    "                \n",
    "                if similarity >= threshold:\n",
    "                    matches.append((record1['patient_id'], record2['patient_id'], similarity))\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_matching_performance(predicted_matches, true_match_dict):\n",
    "    \"\"\"Evaluate the performance of the patient matching algorithm.\n",
    "    \n",
    "    Args:\n",
    "        predicted_matches (list): List of tuples (id1, id2, similarity) for predicted matches\n",
    "        true_match_dict (dict): Dictionary mapping patient IDs to their true match IDs\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with precision, recall, and F1 score\n",
    "    \"\"\"\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    \n",
    "    # Count true positives and false positives\n",
    "    for id1, id2, _ in predicted_matches:\n",
    "        if id1 in true_match_dict and true_match_dict[id1] == id2:\n",
    "            true_positives += 1\n",
    "        elif id2 in true_match_dict and true_match_dict[id2] == id1:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "    \n",
    "    # Count false negatives (true matches that were not predicted)\n",
    "    predicted_pairs = set()\n",
    "    for id1, id2, _ in predicted_matches:\n",
    "        pair = tuple(sorted([id1, id2]))\n",
    "        predicted_pairs.add(pair)\n",
    "    \n",
    "    true_pairs = set()\n",
    "    for id1, id2 in true_match_dict.items():\n",
    "        pair = tuple(sorted([id1, id2]))\n",
    "        true_pairs.add(pair)\n",
    "    \n",
    "    false_negatives = len(true_pairs - predicted_pairs)\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bayesian Optimization for Patient Matching\n",
    "\n",
    "Now we'll implement Bayesian Optimization to find the optimal parameters for our patient matching algorithm. The parameters we'll optimize include:\n",
    "\n",
    "1. Field weights for different demographic attributes\n",
    "2. Similarity threshold for determining matches\n",
    "3. Choice of blocking field (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientMatchingOptimizer:\n",
    "    \"\"\"Bayesian Optimization for patient matching parameters.\n",
    "    \n",
    "    This class implements Bayesian Optimization to find optimal parameters\n",
    "    for patient matching algorithms, maximizing F1 score.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_data, test_data, true_match_dict, n_initial_points=5):\n",
    "        \"\"\"Initialize the optimizer.\n",
    "        \n",
    "        Args:\n",
    "            train_data (pd.DataFrame): Training dataset\n",
    "            test_data (pd.DataFrame): Testing dataset\n",
    "            true_match_dict (dict): Dictionary of true matches\n",
    "            n_initial_points (int): Number of initial random points to evaluate\n",
    "        \"\"\"\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.true_match_dict = true_match_dict\n",
    "        self.n_initial_points = n_initial_points\n",
    "        \n",
    "        # Define parameter bounds\n",
    "        self.bounds = {\n",
    "            'first_name_weight': (0.1, 1.0),\n",
    "            'last_name_weight': (0.1, 1.0),\n",
    "            'birth_date_weight': (0.1, 1.0),\n",
    "            'gender_weight': (0.0, 0.5),\n",
    "            'address_weight': (0.0, 0.8),\n",
    "            'city_weight': (0.0, 0.5),\n",
    "            'state_weight': (0.0, 0.3),\n",
    "            'zip_weight': (0.0, 0.5),\n",
    "            'threshold': (0.5, 0.95)\n",
    "        }\n",
    "        \n",
    "        # Define parameter space for GP\n",
    "        self.param_names = list(self.bounds.keys())\n",
    "        self.param_bounds = np.array([self.bounds[name] for name in self.param_names])\n",
    "        \n",
    "        # Initialize GP model\n",
    "        kernel = ConstantKernel(1.0) * Matern(length_scale=np.ones(len(self.param_names)), nu=2.5)\n",
    "        self.gp = GaussianProcessRegressor(\n",
    "            kernel=kernel,\n",
    "            n_restarts_optimizer=10,\n",
    "            normalize_y=True,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Initialize data storage\n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        \n",
    "        # Define similarity functions for different fields\n",
    "        self.similarity_funcs = {\n",
    "            'first_name': jaro_similarity,\n",
    "            'last_name': jaro_similarity,\n",
    "            'maiden_name': jaro_similarity,\n",
    "            'birth_date': date_similarity,\n",
    "            'gender': lambda x, y: 1.0 if x == y else 0.0,\n",
    "            'address': levenshtein_similarity,\n",
    "            'city': jaro_similarity,\n",
    "            'state': lambda x, y: 1.0 if x == y else 0.0,\n",
    "            'zip': lambda x, y: 1.0 if x == y else 0.0 if len(x) != len(y) else levenshtein_similarity(x, y)\n",
    "        }\n",
    "        \n",
    "        # Generate initial random points\n",
    "        self._evaluate_initial_points()\n",
    "    \n",
    "    def _evaluate_initial_points(self):\n",
    "        \"\"\"Evaluate initial random points to initialize the GP model.\"\"\"\n",
    "        print(\"Evaluating initial random points...\")\n",
    "        \n",
    "        for _ in range(self.n_initial_points):\n",
    "            # Generate random parameters\n",
    "            params = {}\n",
    "            for name, (low, high) in self.bounds.items():\n",
    "                params[name] = np.random.uniform(low, high)\n",
    "            \n",
    "            # Evaluate objective function\n",
    "            score = self._objective_function(params)\n",
    "            \n",
    "            # Store results\n",
    "            self.X_train.append([params[name] for name in self.param_names])\n",
    "            self.y_train.append(score)\n",
    "            \n",
    "            print(f\"Initial point {len(self.X_train)}: F1 score = {score:.4f}\")\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        self.X_train = np.array(self.X_train)\n",
    "        self.y_train = np.array(self.y_train)\n",
    "        \n",
    "        # Fit the GP model\n",
    "        self.gp.fit(self.X_train, self.y_train)\n",
    "    \n",
    "    def _objective_function(self, params):\n",
    "        \"\"\"Objective function to maximize (F1 score of patient matching).\n",
    "        \n",
    "        Args:\n",
    "            params (dict): Dictionary of parameter values\n",
    "            \n",
    "        Returns:\n",
    "            float: F1 score of patient matching with given parameters\n",
    "        \"\"\"\n",
    "        # Extract parameters\n",
    "        field_weights = {\n",
    "            'first_name': params['first_name_weight'],\n",
    "            'last_name': params['last_name_weight'],\n",
    "            'birth_date': params['birth_date_weight'],\n",
    "            'gender': params['gender_weight'],\n",
    "            'address': params['address_weight'],\n",
    "            'city': params['city_weight'],\n",
    "            'state': params['state_weight'],\n",
    "            'zip': params['zip_weight']\n",
    "        }\n",
    "        threshold = params['threshold']\n",
    "        \n",
    "        # Use birth_date as blocking field to improve efficiency\n",
    "        blocking_field = 'birth_date'\n",
    "        \n",
    "        # Find matches on training data\n",
    "        predicted_matches = find_matches(\n",
    "            self.train_data, field_weights, self.similarity_funcs, threshold, blocking_field)\n",
    "        \n",
    "        # Evaluate performance\n",
    "        performance = evaluate_matching_performance(predicted_matches, self.true_match_dict)\n",
    "        \n",
    "        return performance['f1_score']\n",
    "    \n",
    "    def _acquisition_function(self, x, kappa=2.0):\n",
    "        \"\"\"Upper Confidence Bound acquisition function.\n",
    "        \n",
    "        Args:\n",
    "            x (np.array): Point to evaluate\n",
    "            kappa (float): Exploration-exploitation trade-off parameter\n",
    "            \n",
    "        Returns:\n",
    "            float: Acquisition function value\n",
    "        \"\"\"\n",
    "        x = x.reshape(1, -1)\n",
    "        mu, sigma = self.gp.predict(x, return_std=True)\n",
    "        return mu + kappa * sigma\n",
    "    \n",
    "    def optimize(self, n_iterations=10, kappa=2.0):\n",
    "        \"\"\"Run Bayesian Optimization to find optimal parameters.\n",
    "        \n",
    "        Args:\n",
    "            n_iterations (int): Number of optimization iterations\n",
    "            kappa (float): Exploration parameter for acquisition function\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with optimal parameters and performance\n",
    "        \"\"\"\n",
    "        print(f\"\\nStarting Bayesian Optimization for {n_iterations} iterations...\")\n",
    "        \n",
    "        for i in range(n_iterations):\n",
    "            # Find the next point to evaluate by maximizing the acquisition function\n",
    "            best_x = None\n",
    "            best_acquisition = -np.inf\n",
    "            \n",
    "            # Try multiple random starting points\n",
    "            for _ in range(10):\n",
    "                x0 = np.random.uniform(self.param_bounds[:, 0], self.param_bounds[:, 1])\n",
    "                bounds = [(low, high) for low, high in self.param_bounds]\n",
    "                \n",
    "                result = minimize(\n",
    "                    lambda x: -self._acquisition_function(x, kappa),\n",
    "                    x0,\n",
    "                    bounds=bounds,\n",
    "                    method='L-BFGS-B'\n",
    "                )\n",
    "                \n",
    "                if -result.fun > best_acquisition:\n",
    "                    best_acquisition = -result.fun\n",
    "                    best_x = result.x\n",
    "            \n",
    "            # Convert best_x to parameter dictionary\n",
    "            params = {name: best_x[i] for i, name in enumerate(self.param_names)}\n",
    "            \n",
    "            # Evaluate objective function\n",
    "            score = self._objective_function(params)\n",
    "            \n",
    "            # Update data\n",
    "            self.X_train = np.vstack((self.X_train, best_x.reshape(1, -1)))\n",
    "            self.y_train = np.append(self.y_train, score)\n",
    "            \n",
    "            # Update GP model\n",
    "            self.gp.fit(self.X_train, self.y_train)\n",
    "            \n",
    "            print(f\"Iteration {i+1}: F1 score = {score:.4f}\")\n",
    "            \n",
    "            # Print current best parameters\n",
    "            best_idx = np.argmax(self.y_train)\n",
    "            best_score = self.y_train[best_idx]\n",
    "            best_params = {name: self.X_train[best_idx, i] for i, name in enumerate(self.param_names)}\n",
    "            \n",
    "            print(f\"Current best F1 score: {best_score:.4f}\")\n",
    "        \n",
    "        # Get final best parameters\n",
    "        best_idx = np.argmax(self.y_train)\n",
    "        best_score = self.y_train[best_idx]\n",
    "        best_params = {name: self.X_train[best_idx, i] for i, name in enumerate(self.param_names)}\n",
    "        \n",
    "        print(f\"\\nOptimization complete!\")\n",
    "        print(f\"Best F1 score: {best_score:.4f}\")\n",
    "        print(\"Best parameters:\")\n",
    "        for name, value in best_params.items():\n",
    "            print(f\"  {name}: {value:.4f}\")\n",
    "        \n",
    "        # Evaluate on test data\n",
    "        field_weights = {k: v for k, v in best_params.items() if k != 'threshold'}\n",
    "        threshold = best_params['threshold']\n",
    "        \n",
    "        predicted_matches = find_matches(\n",
    "            self.test_data, field_weights, self.similarity_funcs, threshold, 'birth_date')\n",
    "        \n",
    "        test_performance = evaluate_matching_performance(predicted_matches, self.true_match_dict)\n",
    "        \n",
    "        print(f\"\\nTest performance:\")\n",
    "        print(f\"  Precision: {test_performance['precision']:.4f}\")\n",
    "        print(f\"  Recall: {test_performance['recall']:.4f}\")\n",
    "        print(f\"  F1 score: {test_performance['f1_score']:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'best_params': best_params,\n",
    "            'train_score': best_score,\n",
    "            'test_performance': test_performance,\n",
    "            'optimization_history': {\n",
    "                'X': self.X_train,\n",
    "                'y': self.y_train\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Bayesian Optimization\n",
    "optimizer = PatientMatchingOptimizer(train_data, test_data, true_match_dict, n_initial_points=5)\n",
    "optimization_results = optimizer.optimize(n_iterations=15, kappa=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization and Analysis of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(optimizer.y_train) + 1), optimizer.y_train, 'o-', color='blue')\n",
    "plt.axhline(y=max(optimizer.y_train), color='red', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Bayesian Optimization Progress')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Plot parameter importance\n",
    "best_params = optimization_results['best_params']\n",
    "param_names = [name for name in best_params.keys() if name != 'threshold']\n",
    "param_values = [best_params[name] for name in param_names]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(param_names, param_values, color='skyblue')\n",
    "plt.xlabel('Parameter')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Optimal Field Weights for Patient Matching')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the impact of threshold on performance\n",
    "thresholds = np.linspace(0.5, 0.95, 10)\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Get the best field weights\n",
    "field_weights = {k: v for k, v in best_params.items() if k != 'threshold'}\n",
    "similarity_funcs = optimizer.similarity_funcs\n",
    "\n",
    "for threshold in thresholds:\n",
    "    predicted_matches = find_matches(\n",
    "        test_data, field_weights, similarity_funcs, threshold, 'birth_date')\n",
    "    \n",
    "    performance = evaluate_matching_performance(predicted_matches, true_match_dict)\n",
    "    \n",
    "    precision_scores.append(performance['precision'])\n",
    "    recall_scores.append(performance['recall'])\n",
    "    f1_scores.append(performance['f1_score'])\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, precision_scores, 'o-', color='blue', label='Precision')\n",
    "plt.plot(thresholds, recall_scores, 'o-', color='green', label='Recall')\n",
    "plt.plot(thresholds, f1_scores, 'o-', color='red', label='F1 Score')\n",
    "\n",
    "# Mark the optimal threshold\n",
    "optimal_threshold = best_params['threshold']\n",
    "plt.axvline(x=optimal_threshold, color='purple', linestyle='--', alpha=0.5,\n",
    "            label=f'Optimal Threshold: {optimal_threshold:.2f}')\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Impact of Threshold on Matching Performance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Applying the Optimized Patient Matching Algorithm\n",
    "\n",
    "Now we'll apply the optimized patient matching algorithm to a sample of patient records to demonstrate its effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample of test records\n",
    "sample_size = min(10, len(test_data))\n",
    "sample_indices = random.sample(range(len(test_data)), sample_size)\n",
    "sample_records = test_data.iloc[sample_indices].copy()\n",
    "\n",
    "# Apply the optimized matching algorithm\n",
    "field_weights = {k: v for k, v in best_params.items() if k != 'threshold'}\n",
    "threshold = best_params['threshold']\n",
    "\n",
    "predicted_matches = find_matches(\n",
    "    test_data, field_weights, similarity_funcs, threshold, 'birth_date')\n",
    "\n",
    "# Display sample records and their matches\n",
    "print(\"Sample Records and Their Matches:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for _, record in sample_records.iterrows():\n",
    "    record_id = record['patient_id']\n",
    "    print(f\"Record ID: {record_id}\")\n",
    "    print(f\"Name: {record['first_name']} {record['last_name']}\")\n",
    "    print(f\"Birth Date: {record['birth_date']}\")\n",
    "    print(f\"Gender: {record['gender']}\")\n",
    "    print(f\"Address: {record['address']}, {record['city']}, {record['state']} {record['zip']}\")\n",
    "    \n",
    "    # Find matches for this record\n",
    "    matches = []\n",
    "    for id1, id2, similarity in predicted_matches:\n",
    "        if id1 == record_id:\n",
    "            matches.append((id2, similarity))\n",
    "        elif id2 == record_id:\n",
    "            matches.append((id1, similarity))\n",
    "    \n",
    "    if matches:\n",
    "        print(\"\\nMatches:\")\n",
    "        for match_id, similarity in matches:\n",
    "            match_record = test_data[test_data['patient_id'] == match_id].iloc[0]\n",
    "            print(f\"  Match ID: {match_id} (Similarity: {similarity:.4f})\")\n",
    "            print(f\"  Name: {match_record['first_name']} {match_record['last_name']}\")\n",
    "            print(f\"  Birth Date: {match_record['birth_date']}\")\n",
    "            print(f\"  Gender: {match_record['gender']}\")\n",
    "            print(f\"  Address: {match_record['address']}, {match_record['city']}, {match_record['state']} {match_record['zip']}\")\n",
    "            \n",
    "            # Check if this is a true match\n",
    "            is_true_match = (record_id in true_match_dict and true_match_dict[record_id] == match_id) or \\\n",
    "                           (match_id in true_match_dict and true_match_dict[match_id] == record_id)\n",
    "            \n",
    "            if is_true_match:\n",
    "                print(\"   TRUE MATCH\")\n",
    "            else:\n",
    "                print(\"   FALSE MATCH\")\n",
    "    else:\n",
    "        print(\"\\nNo matches found.\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we demonstrated the application of Bayesian Optimization to tune patient matching algorithms for healthcare interoperability. By optimizing field weights and similarity thresholds, we achieved significant improvements in matching accuracy compared to default configurations.\n",
    "\n",
    "Key findings:\n",
    "\n",
    "1. The most important fields for patient matching were found to be last name, first name, and birth date, which aligns with domain knowledge in healthcare.\n",
    "\n",
    "2. Bayesian Optimization efficiently explored the parameter space, finding an optimal configuration in relatively few iterations compared to grid search or random search approaches.\n",
    "\n",
    "3. The precision-recall tradeoff can be controlled through the similarity threshold, with the optimal threshold balancing false positives and false negatives.\n",
    "\n",
    "This approach can be extended to real-world eMPI systems, where optimizing patient matching parameters is critical for ensuring accurate patient identification across healthcare systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
